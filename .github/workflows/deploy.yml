name: Deploy to AWS ECS

on:
  push:
    branches: [main]
    paths:
      - "backend/**"
      - "frontend/**"
      - "infra/**"
  workflow_dispatch:
    inputs:
      deploy_mode:
        description: "Target environment mode for Terraform/app config"
        required: false
        default: production
        type: choice
        options:
          - sandbox
          - production
      tf_state_key:
        description: "Terraform backend state key override (example: sandbox/terraform.tfstate)"
        required: false
        default: ""
        type: string
      force_unlock:
        description: "Force-unlock Terraform state lock before plan/apply (manual runs only)"
        required: false
        default: false
        type: boolean
      force_unlock_id:
        description: "Terraform lock ID to force-unlock (required when force_unlock=true)"
        required: false
        default: ""
        type: string

concurrency:
  # Prevent parallel deploy workflow runs from competing on the same state lock.
  group: deploy-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: us-east-1
  PROJECT_NAME: arbiter-ai
  DEFAULT_DEPLOY_MODE: production
  DEFAULT_TF_STATE_KEY: prod/terraform.tfstate

jobs:
  changes:
    name: Detect Changes
    runs-on: ubuntu-latest
    outputs:
      backend: ${{ steps.filter.outputs.backend }}
      frontend: ${{ steps.filter.outputs.frontend }}
      infra: ${{ steps.filter.outputs.infra }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          base: main
          filters: |
            backend:
              - 'backend/**'
            frontend:
              - 'frontend/**'
            infra:
              - 'infra/**'

  infra-prebuild-check:
    name: Infra Pre-Build Check
    runs-on: ubuntu-latest
    needs: changes
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "~> 1.5"

      - name: Validate infra inventory + Terraform config
        run: |
          python3 infra/scripts/check_infra_inventory.py --check
          terraform -chdir=infra/terraform fmt -check -recursive || {
            echo "::error::Terraform formatting drift detected. Run: docker run --rm -v \"$PWD\":/work -w /work hashicorp/terraform:1.5.7 fmt -recursive infra/terraform"
            terraform -chdir=infra/terraform fmt -diff -recursive
            exit 1
          }
          terraform -chdir=infra/terraform init -backend=false
          terraform -chdir=infra/terraform validate

  build-backend:
    name: Build & Push Backend
    runs-on: ubuntu-latest
    needs: [changes, infra-prebuild-check]
    if: needs.changes.outputs.backend == 'true'
    outputs:
      image_tag: ${{ steps.meta.outputs.tag }}
    steps:
      - uses: actions/checkout@v4

      - name: Set image tag
        id: meta
        run: echo "tag=sha-$(git rev-parse --short HEAD)" >> "$GITHUB_OUTPUT"

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: ecr-login
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build & push backend image
        env:
          REGISTRY: ${{ steps.ecr-login.outputs.registry }}
          TAG: ${{ steps.meta.outputs.tag }}
        run: |
          docker build -t $REGISTRY/$PROJECT_NAME-backend:$TAG -t $REGISTRY/$PROJECT_NAME-backend:latest -f backend/Dockerfile backend/
          docker push $REGISTRY/$PROJECT_NAME-backend:$TAG
          docker push $REGISTRY/$PROJECT_NAME-backend:latest

  build-frontend:
    name: Build & Push Frontend
    runs-on: ubuntu-latest
    needs: [changes, infra-prebuild-check]
    if: needs.changes.outputs.frontend == 'true'
    outputs:
      image_tag: ${{ steps.meta.outputs.tag }}
    steps:
      - uses: actions/checkout@v4

      - name: Set image tag
        id: meta
        run: echo "tag=sha-$(git rev-parse --short HEAD)" >> "$GITHUB_OUTPUT"

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: ecr-login
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build & push frontend image
        env:
          REGISTRY: ${{ steps.ecr-login.outputs.registry }}
          TAG: ${{ steps.meta.outputs.tag }}
          NEXT_PUBLIC_API_URL: ${{ secrets.NEXT_PUBLIC_API_URL || '/api/v1' }}
        run: |
          docker build --build-arg NEXT_PUBLIC_API_URL="$NEXT_PUBLIC_API_URL" -t $REGISTRY/$PROJECT_NAME-frontend:$TAG -t $REGISTRY/$PROJECT_NAME-frontend:latest -f frontend/Dockerfile frontend/
          docker push $REGISTRY/$PROJECT_NAME-frontend:$TAG
          docker push $REGISTRY/$PROJECT_NAME-frontend:latest

  terraform:
    name: Terraform Apply
    runs-on: ubuntu-latest
    needs: [changes, infra-prebuild-check, build-backend, build-frontend]
    if: always() && !cancelled()
    defaults:
      run:
        working-directory: infra/terraform
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "~> 1.5"

      - name: Validate required Terraform secrets
        env:
          SECRETS_MANAGER_ARN: ${{ secrets.SECRETS_MANAGER_ARN }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
        run: |
          MISSING=0
          if [ -z "$SECRETS_MANAGER_ARN" ]; then
            echo "::error::Missing required GitHub secret: SECRETS_MANAGER_ARN"
            MISSING=1
          fi
          if [ -z "$DB_PASSWORD" ]; then
            echo "::error::Missing required GitHub secret: DB_PASSWORD"
            MISSING=1
          fi
          if [ "$MISSING" -ne 0 ]; then
            exit 1
          fi

      - name: Resolve deploy context
        id: context
        run: |
          DEPLOY_MODE="${{ github.event.inputs.deploy_mode }}"
          if [ -z "$DEPLOY_MODE" ]; then
            DEPLOY_MODE="${{ vars.DEPLOY_MODE }}"
          fi
          if [ -z "$DEPLOY_MODE" ]; then
            DEPLOY_MODE="${DEFAULT_DEPLOY_MODE}"
          fi
          DEPLOY_MODE="$(echo "$DEPLOY_MODE" | tr '[:upper:]' '[:lower:]' | xargs)"
          if [ "$DEPLOY_MODE" = "prod" ]; then
            echo "Normalizing DEPLOY_MODE=prod to production."
            DEPLOY_MODE="production"
          fi
          if [ "$DEPLOY_MODE" != "production" ] && [ "$DEPLOY_MODE" != "sandbox" ]; then
            echo "::error::Unsupported deploy mode: $DEPLOY_MODE (expected sandbox or production)."
            exit 1
          fi

          TF_STATE_KEY="${{ github.event.inputs.tf_state_key }}"
          if [ -z "$TF_STATE_KEY" ]; then
            if [ "$DEPLOY_MODE" = "production" ]; then
              TF_STATE_KEY="${DEFAULT_TF_STATE_KEY}"
            else
              TF_STATE_KEY="${DEPLOY_MODE}/terraform.tfstate"
            fi
          fi
          if [ "$DEPLOY_MODE" = "production" ] && [ "$TF_STATE_KEY" = "production/terraform.tfstate" ]; then
            echo "Normalizing legacy production state key to ${DEFAULT_TF_STATE_KEY}."
            TF_STATE_KEY="${DEFAULT_TF_STATE_KEY}"
          fi

          echo "deploy_mode=${DEPLOY_MODE}" >> "$GITHUB_OUTPUT"
          echo "tf_state_key=${TF_STATE_KEY}" >> "$GITHUB_OUTPUT"
          TF_VARS_FILE="environments/${DEPLOY_MODE}.tfvars"
          if [ -f "$TF_VARS_FILE" ]; then
            echo "tf_vars_file=${TF_VARS_FILE}" >> "$GITHUB_OUTPUT"
          else
            echo "tf_vars_file=" >> "$GITHUB_OUTPUT"
          fi

      - name: Terraform Init
        run: terraform init -backend-config="key=${{ steps.context.outputs.tf_state_key }}"

      - name: Validate deploy tfvars profile
        if: ${{ steps.context.outputs.tf_vars_file == '' }}
        run: |
          echo "::error::Deploy mode '${{ steps.context.outputs.deploy_mode }}' requires infra/terraform/environments/${{ steps.context.outputs.deploy_mode }}.tfvars to avoid destructive default drift."
          exit 1

      - name: Validate production state key
        if: ${{ steps.context.outputs.deploy_mode == 'production' }}
        run: |
          if [ "${{ steps.context.outputs.tf_state_key }}" != "${DEFAULT_TF_STATE_KEY}" ]; then
            echo "::error::Production deploys must use ${DEFAULT_TF_STATE_KEY}. Got '${{ steps.context.outputs.tf_state_key }}'."
            exit 1
          fi

      - name: Validate manual force-unlock inputs
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.force_unlock == 'true' && github.event.inputs.force_unlock_id == '' }}
        run: |
          echo "::error::force_unlock=true requires force_unlock_id to be set."
          exit 1

      - name: Optional manual force-unlock
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.force_unlock == 'true' && github.event.inputs.force_unlock_id != '' }}
        run: |
          echo "Manual force-unlock requested for state key: ${{ steps.context.outputs.tf_state_key }}"
          terraform force-unlock -force "${{ github.event.inputs.force_unlock_id }}"

      - name: Prune unmanaged SES domain state (production)
        if: ${{ steps.context.outputs.deploy_mode == 'production' && steps.context.outputs.tf_vars_file != '' }}
        run: |
          # If production tfvars sets ses_domain="", SES identity resources are intentionally unmanaged.
          # Remove any legacy SES domain resources from state so plan does not fail on read permissions.
          if grep -Eq '^[[:space:]]*ses_domain[[:space:]]*=[[:space:]]*""[[:space:]]*$' "${{ steps.context.outputs.tf_vars_file }}"; then
            echo "ses_domain is empty in ${{ steps.context.outputs.tf_vars_file }}; pruning legacy SES domain resources from Terraform state (best effort)."
            SES_STATE_ADDRS="$(terraform state list 2>/dev/null | grep '^aws_ses_domain_' || true)"
            if [ -n "$SES_STATE_ADDRS" ]; then
              while IFS= read -r ADDR; do
                if [ -n "$ADDR" ]; then
                  echo "Pruning SES state address: $ADDR"
                  terraform state rm "$ADDR" || true
                fi
              done <<< "$SES_STATE_ADDRS"
            else
              echo "No legacy SES state addresses found."
            fi
          fi

      - name: Import existing ECS services
        env:
          # Import evaluates the Terraform configuration, so required vars must
          # be present here too (not only in terraform plan/apply).
          TF_VAR_environment: ${{ steps.context.outputs.deploy_mode }}
          TF_VAR_app_mode: ${{ steps.context.outputs.deploy_mode }}
          TF_VAR_app_env: ${{ steps.context.outputs.deploy_mode }}
          TF_VAR_secrets_manager_arn: ${{ secrets.SECRETS_MANAGER_ARN }}
          TF_VAR_db_password: ${{ secrets.DB_PASSWORD }}
        run: |
          # Best-effort import: If services exist but aren't in state, bring them in.
          # If they don't exist (fresh deploy) or are already in state, this fails safely (|| true).
          # The subsequent 'plan' will determine the actual required changes.
          IMPORT_ARGS=()
          if [ -n "${{ steps.context.outputs.tf_vars_file }}" ]; then
            IMPORT_ARGS+=("-var-file=${{ steps.context.outputs.tf_vars_file }}")
          fi
          PROJECT_NAME="${{ env.PROJECT_NAME }}"
          for svc in backend frontend worker beat; do
            echo "Attempting import for aws_ecs_service.${svc}..."
            terraform import "${IMPORT_ARGS[@]}" -lock-timeout=10m "aws_ecs_service.${svc}" "${PROJECT_NAME}-cluster/${PROJECT_NAME}-${svc}" || true
          done

      - name: Determine image tags
        id: tags
        run: |
          if [ -n "${{ needs.build-backend.outputs.image_tag }}" ]; then
            echo "backend_tag=${{ needs.build-backend.outputs.image_tag }}" >> "$GITHUB_OUTPUT"
          else
            echo "backend_tag=latest" >> "$GITHUB_OUTPUT"
          fi
          if [ -n "${{ needs.build-frontend.outputs.image_tag }}" ]; then
            echo "frontend_tag=${{ needs.build-frontend.outputs.image_tag }}" >> "$GITHUB_OUTPUT"
          else
            echo "frontend_tag=latest" >> "$GITHUB_OUTPUT"
          fi

      - name: Terraform Plan
        run: |
          PLAN_ARGS=()
          if [ -n "${{ steps.context.outputs.tf_vars_file }}" ]; then
            PLAN_ARGS+=("-var-file=${{ steps.context.outputs.tf_vars_file }}")
          fi
          run_plan() {
            terraform plan "${PLAN_ARGS[@]}" \
              -no-color \
              -lock-timeout=10m \
              -var="environment=${{ steps.context.outputs.deploy_mode }}" \
              -var="app_mode=${{ steps.context.outputs.deploy_mode }}" \
              -var="app_env=${{ steps.context.outputs.deploy_mode }}" \
              -var="backend_image_tag=${{ steps.tags.outputs.backend_tag }}" \
              -var="frontend_image_tag=${{ steps.tags.outputs.frontend_tag }}" \
              -var="secrets_manager_arn=${{ secrets.SECRETS_MANAGER_ARN }}" \
              -var="db_password=${{ secrets.DB_PASSWORD }}" \
              -out=tfplan
          }

          set +e
          run_plan 2>&1 | tee /tmp/terraform-plan.log
          PLAN_EXIT=${PIPESTATUS[0]}
          set -e

          if [ "$PLAN_EXIT" -eq 0 ]; then
            exit 0
          fi

          # Recovery path for stale DynamoDB lock rows from interrupted runs.
          if grep -q "Error acquiring the state lock" /tmp/terraform-plan.log; then
            LOCK_ID="$(grep -m1 "ID:" /tmp/terraform-plan.log | sed -E 's/.*ID:[[:space:]]*([^[:space:]]+).*/\1/')"
            if [ -n "$LOCK_ID" ]; then
              echo "Detected Terraform state lock ($LOCK_ID). Attempting force-unlock then one retry."
              terraform force-unlock -force "$LOCK_ID" || true
              run_plan
              exit 0
            fi
          fi

          exit "$PLAN_EXIT"

      - name: Terraform Apply
        run: terraform apply -lock-timeout=10m -auto-approve tfplan

  deploy:
    name: Deploy Services
    runs-on: ubuntu-latest
    needs: [changes, build-backend, build-frontend, terraform]
    if: always() && needs.terraform.result == 'success' && (needs.build-backend.result == 'success' || needs.build-frontend.result == 'success')
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Deploy updated backend
        if: needs.build-backend.result == 'success'
        run: |
          aws ecs update-service --cluster $PROJECT_NAME-cluster --service $PROJECT_NAME-backend --force-new-deployment
          aws ecs update-service --cluster $PROJECT_NAME-cluster --service $PROJECT_NAME-worker --force-new-deployment
          aws ecs update-service --cluster $PROJECT_NAME-cluster --service $PROJECT_NAME-beat --force-new-deployment

      - name: Deploy updated frontend
        if: needs.build-frontend.result == 'success'
        run: |
          aws ecs update-service --cluster $PROJECT_NAME-cluster --service $PROJECT_NAME-frontend --force-new-deployment

      - name: Wait for services to stabilize
        run: |
          SERVICES_TO_WAIT=()
          if [ "${{ needs.build-backend.result }}" = "success" ]; then
            SERVICES_TO_WAIT+=("$PROJECT_NAME-backend" "$PROJECT_NAME-worker" "$PROJECT_NAME-beat")
          fi
          if [ "${{ needs.build-frontend.result }}" = "success" ]; then
            SERVICES_TO_WAIT+=("$PROJECT_NAME-frontend")
          fi

          if [ "${#SERVICES_TO_WAIT[@]}" -eq 0 ]; then
            echo "No updated services to wait for."
            exit 0
          fi

          echo "Waiting for ECS services: ${SERVICES_TO_WAIT[*]}"
          aws ecs describe-services \
            --cluster "$PROJECT_NAME-cluster" \
            --services "${SERVICES_TO_WAIT[@]}" \
            --query "services[].{service:serviceName,desired:desiredCount,running:runningCount,pending:pendingCount,events:events[0].message}" \
            --output table

          run_wait() {
            aws ecs wait services-stable --cluster "$PROJECT_NAME-cluster" --services "${SERVICES_TO_WAIT[@]}"
          }

          set +e
          run_wait
          WAIT_EXIT=$?
          set -e

          if [ "$WAIT_EXIT" -ne 0 ]; then
            echo "First stabilization wait failed; retrying once after 20s."
            sleep 20
            set +e
            run_wait
            WAIT_EXIT=$?
            set -e
          fi

          if [ "$WAIT_EXIT" -ne 0 ]; then
            echo "::error::ECS services failed to stabilize."
            aws ecs describe-services \
              --cluster "$PROJECT_NAME-cluster" \
              --services "${SERVICES_TO_WAIT[@]}" \
              --query "services[].{service:serviceName,status:status,desired:desiredCount,running:runningCount,pending:pendingCount,deployments:deployments[*].{rolloutState:rolloutState,status:status,desired:desiredCount,running:runningCount,pending:pendingCount},events:events[0:5].[createdAt,message]}" \
              --output json
            exit "$WAIT_EXIT"
          fi

      - name: Resolve ALB endpoint
        id: alb
        run: |
          ALB_DNS="$(aws elbv2 describe-load-balancers --names "$PROJECT_NAME-alb" --query "LoadBalancers[0].DNSName" --output text)"
          if [ -z "$ALB_DNS" ] || [ "$ALB_DNS" = "None" ]; then
            echo "::error::Unable to resolve ALB DNS for $PROJECT_NAME-alb."
            exit 1
          fi
          echo "dns=${ALB_DNS}" >> "$GITHUB_OUTPUT"
          echo "Resolved ALB DNS: ${ALB_DNS}"

      - name: Bootstrap catalog if empty
        env:
          CATALOG_URL: http://${{ steps.alb.outputs.dns }}/api/v1/catalog/
        run: |
          catalog_count() {
            python3 - <<'PY'
          import json
          import os
          import urllib.request

          url = os.environ["CATALOG_URL"]
          try:
              with urllib.request.urlopen(url, timeout=20) as response:
                  payload = json.loads(response.read().decode("utf-8"))
          except Exception:
              print(-1)
          else:
              print(len(payload) if isinstance(payload, list) else -1)
          PY
          }

          EXISTING_COUNT="$(catalog_count)"
          if [ "$EXISTING_COUNT" -lt 0 ]; then
            echo "::error::Failed to read catalog endpoint at ${CATALOG_URL}."
            exit 1
          fi

          if [ "$EXISTING_COUNT" -gt 0 ]; then
            echo "Catalog already populated (${EXISTING_COUNT} entries); skipping bootstrap."
            exit 0
          fi

          echo "Catalog is empty; running one-off backend bootstrap task (scripts.seed_catalog)."
          TASK_DEFINITION="$(aws ecs describe-services \
            --cluster "$PROJECT_NAME-cluster" \
            --services "$PROJECT_NAME-backend" \
            --query "services[0].taskDefinition" \
            --output text)"
          SUBNETS_JSON="$(aws ecs describe-services \
            --cluster "$PROJECT_NAME-cluster" \
            --services "$PROJECT_NAME-backend" \
            --query "services[0].networkConfiguration.awsvpcConfiguration.subnets" \
            --output json)"
          SECURITY_GROUPS_JSON="$(aws ecs describe-services \
            --cluster "$PROJECT_NAME-cluster" \
            --services "$PROJECT_NAME-backend" \
            --query "services[0].networkConfiguration.awsvpcConfiguration.securityGroups" \
            --output json)"
          ASSIGN_PUBLIC_IP="$(aws ecs describe-services \
            --cluster "$PROJECT_NAME-cluster" \
            --services "$PROJECT_NAME-backend" \
            --query "services[0].networkConfiguration.awsvpcConfiguration.assignPublicIp" \
            --output text)"

          if [ -z "$TASK_DEFINITION" ] || [ "$TASK_DEFINITION" = "None" ]; then
            echo "::error::Unable to resolve backend task definition for bootstrap."
            exit 1
          fi

          NETWORK_CONFIGURATION="$(
            SUBNETS_JSON="$SUBNETS_JSON" \
            SECURITY_GROUPS_JSON="$SECURITY_GROUPS_JSON" \
            ASSIGN_PUBLIC_IP="$ASSIGN_PUBLIC_IP" \
            python3 - <<'PY'
          import json
          import os

          network = {
              "awsvpcConfiguration": {
                  "subnets": json.loads(os.environ["SUBNETS_JSON"]),
                  "securityGroups": json.loads(os.environ["SECURITY_GROUPS_JSON"]),
                  "assignPublicIp": os.environ["ASSIGN_PUBLIC_IP"],
              }
          }
          print(json.dumps(network))
          PY
          )"

          RUN_TASK_OUTPUT="$(
            aws ecs run-task \
              --cluster "$PROJECT_NAME-cluster" \
              --launch-type FARGATE \
              --task-definition "$TASK_DEFINITION" \
              --network-configuration "$NETWORK_CONFIGURATION" \
              --overrides '{"containerOverrides":[{"name":"backend","command":["python","-m","scripts.seed_catalog"]}]}' \
              --count 1 \
              --output json
          )"

          TASK_ARN="$(
            RUN_TASK_OUTPUT="$RUN_TASK_OUTPUT" python3 - <<'PY'
          import json
          import os

          payload = json.loads(os.environ["RUN_TASK_OUTPUT"])
          task_arn = ""
          tasks = payload.get("tasks") or []
          if tasks:
              task_arn = tasks[0].get("taskArn") or ""
          print(task_arn)
          PY
          )"

          if [ -z "$TASK_ARN" ]; then
            echo "::error::Failed to launch catalog bootstrap task."
            echo "$RUN_TASK_OUTPUT"
            exit 1
          fi

          echo "Waiting for bootstrap task to finish: ${TASK_ARN}"
          aws ecs wait tasks-stopped --cluster "$PROJECT_NAME-cluster" --tasks "$TASK_ARN"

          EXIT_CODE="$(aws ecs describe-tasks \
            --cluster "$PROJECT_NAME-cluster" \
            --tasks "$TASK_ARN" \
            --query "tasks[0].containers[?name==\`backend\`].exitCode | [0]" \
            --output text)"
          STOPPED_REASON="$(aws ecs describe-tasks \
            --cluster "$PROJECT_NAME-cluster" \
            --tasks "$TASK_ARN" \
            --query "tasks[0].stoppedReason" \
            --output text)"
          CONTAINER_REASON="$(aws ecs describe-tasks \
            --cluster "$PROJECT_NAME-cluster" \
            --tasks "$TASK_ARN" \
            --query "tasks[0].containers[?name==\`backend\`].reason | [0]" \
            --output text)"

          if [ "$EXIT_CODE" != "0" ]; then
            echo "::error::Catalog bootstrap task failed (exit code: $EXIT_CODE)."
            echo "stoppedReason=$STOPPED_REASON"
            echo "containerReason=$CONTAINER_REASON"
            exit 1
          fi

      - name: Smoke check catalog is populated
        env:
          CATALOG_URL: http://${{ steps.alb.outputs.dns }}/api/v1/catalog/
        run: |
          catalog_count() {
            python3 - <<'PY'
          import json
          import os
          import urllib.request

          url = os.environ["CATALOG_URL"]
          try:
              with urllib.request.urlopen(url, timeout=20) as response:
                  payload = json.loads(response.read().decode("utf-8"))
          except Exception:
              print(-1)
          else:
              print(len(payload) if isinstance(payload, list) else -1)
          PY
          }

          ATTEMPTS=12
          SLEEP_SECONDS=15

          for ATTEMPT in $(seq 1 "$ATTEMPTS"); do
            COUNT="$(catalog_count)"
            if [ "$COUNT" -gt 0 ]; then
              echo "Catalog ready with ${COUNT} entries."
              exit 0
            fi
            if [ "$COUNT" -lt 0 ]; then
              echo "Catalog check attempt ${ATTEMPT}/${ATTEMPTS} failed (endpoint unavailable)."
            else
              echo "Catalog still empty on attempt ${ATTEMPT}/${ATTEMPTS}."
            fi
            if [ "$ATTEMPT" -lt "$ATTEMPTS" ]; then
              sleep "$SLEEP_SECONDS"
            fi
          done

          echo "::error::Catalog is still empty after deploy bootstrap and retries."
          exit 1
