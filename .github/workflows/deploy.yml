name: Deploy to AWS ECS

on:
  push:
    branches: [main]
    paths:
      - "backend/**"
      - "frontend/**"
      - "infra/**"
  workflow_dispatch:
    inputs:
      deploy_mode:
        description: "Target environment mode for Terraform/app config"
        required: false
        default: production
        type: choice
        options:
          - sandbox
          - production
      tf_state_key:
        description: "Terraform backend state key override (example: sandbox/terraform.tfstate)"
        required: false
        default: ""
        type: string
      force_unlock:
        description: "Force-unlock Terraform state lock before plan/apply (manual runs only)"
        required: false
        default: false
        type: boolean
      force_unlock_id:
        description: "Terraform lock ID to force-unlock (required when force_unlock=true)"
        required: false
        default: ""
        type: string

concurrency:
  # Prevent parallel deploy workflow runs from competing on the same state lock.
  group: deploy-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: us-east-1
  PROJECT_NAME: arbiter-ai
  DEFAULT_DEPLOY_MODE: production
  DEFAULT_TF_STATE_KEY: prod/terraform.tfstate

jobs:
  changes:
    name: Detect Changes
    runs-on: ubuntu-latest
    outputs:
      backend: ${{ steps.filter.outputs.backend }}
      frontend: ${{ steps.filter.outputs.frontend }}
      infra: ${{ steps.filter.outputs.infra }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          base: main
          filters: |
            backend:
              - 'backend/**'
            frontend:
              - 'frontend/**'
            infra:
              - 'infra/**'

  infra-prebuild-check:
    name: Infra Pre-Build Check
    runs-on: ubuntu-latest
    needs: changes
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "~> 1.5"

      - name: Validate infra inventory + Terraform config
        run: |
          python3 infra/scripts/check_infra_inventory.py --check
          terraform -chdir=infra/terraform fmt -check -recursive || {
            echo "::error::Terraform formatting drift detected. Run: docker run --rm -v \"$PWD\":/work -w /work hashicorp/terraform:1.5.7 fmt -recursive infra/terraform"
            terraform -chdir=infra/terraform fmt -diff -recursive
            exit 1
          }
          terraform -chdir=infra/terraform init -backend=false
          terraform -chdir=infra/terraform validate

  build-backend:
    name: Build & Push Backend
    runs-on: ubuntu-latest
    needs: [changes, infra-prebuild-check]
    if: needs.changes.outputs.backend == 'true'
    outputs:
      image_tag: ${{ steps.meta.outputs.tag }}
    steps:
      - uses: actions/checkout@v4

      - name: Set image tag
        id: meta
        run: echo "tag=sha-$(git rev-parse --short HEAD)" >> "$GITHUB_OUTPUT"

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: ecr-login
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build & push backend image
        env:
          REGISTRY: ${{ steps.ecr-login.outputs.registry }}
          TAG: ${{ steps.meta.outputs.tag }}
        run: |
          docker build -t $REGISTRY/$PROJECT_NAME-backend:$TAG -t $REGISTRY/$PROJECT_NAME-backend:latest -f backend/Dockerfile backend/
          docker push $REGISTRY/$PROJECT_NAME-backend:$TAG
          docker push $REGISTRY/$PROJECT_NAME-backend:latest

  build-frontend:
    name: Build & Push Frontend
    runs-on: ubuntu-latest
    needs: [changes, infra-prebuild-check]
    if: needs.changes.outputs.frontend == 'true'
    outputs:
      image_tag: ${{ steps.meta.outputs.tag }}
    steps:
      - uses: actions/checkout@v4

      - name: Set image tag
        id: meta
        run: echo "tag=sha-$(git rev-parse --short HEAD)" >> "$GITHUB_OUTPUT"

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: ecr-login
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build & push frontend image
        env:
          REGISTRY: ${{ steps.ecr-login.outputs.registry }}
          TAG: ${{ steps.meta.outputs.tag }}
          NEXT_PUBLIC_API_URL: ${{ secrets.NEXT_PUBLIC_API_URL || '/api/v1' }}
        run: |
          docker build --build-arg NEXT_PUBLIC_API_URL="$NEXT_PUBLIC_API_URL" -t $REGISTRY/$PROJECT_NAME-frontend:$TAG -t $REGISTRY/$PROJECT_NAME-frontend:latest -f frontend/Dockerfile frontend/
          docker push $REGISTRY/$PROJECT_NAME-frontend:$TAG
          docker push $REGISTRY/$PROJECT_NAME-frontend:latest

  terraform:
    name: Terraform Apply
    runs-on: ubuntu-latest
    needs: [changes, infra-prebuild-check, build-backend, build-frontend]
    if: always() && !cancelled()
    defaults:
      run:
        working-directory: infra/terraform
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "~> 1.5"

      - name: Validate required Terraform secrets
        env:
          SECRETS_MANAGER_ARN: ${{ secrets.SECRETS_MANAGER_ARN }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
        run: |
          MISSING=0
          if [ -z "$SECRETS_MANAGER_ARN" ]; then
            echo "::error::Missing required GitHub secret: SECRETS_MANAGER_ARN"
            MISSING=1
          fi
          if [ -z "$DB_PASSWORD" ]; then
            echo "::error::Missing required GitHub secret: DB_PASSWORD"
            MISSING=1
          fi
          if [ "$MISSING" -ne 0 ]; then
            exit 1
          fi

      - name: Resolve deploy context
        id: context
        run: |
          DEPLOY_MODE="${{ github.event.inputs.deploy_mode }}"
          if [ -z "$DEPLOY_MODE" ]; then
            DEPLOY_MODE="${{ vars.DEPLOY_MODE }}"
          fi
          if [ -z "$DEPLOY_MODE" ]; then
            DEPLOY_MODE="${DEFAULT_DEPLOY_MODE}"
          fi
          DEPLOY_MODE="$(echo "$DEPLOY_MODE" | tr '[:upper:]' '[:lower:]' | xargs)"
          if [ "$DEPLOY_MODE" = "prod" ]; then
            echo "Normalizing DEPLOY_MODE=prod to production."
            DEPLOY_MODE="production"
          fi
          if [ "$DEPLOY_MODE" != "production" ] && [ "$DEPLOY_MODE" != "sandbox" ]; then
            echo "::error::Unsupported deploy mode: $DEPLOY_MODE (expected sandbox or production)."
            exit 1
          fi

          TF_STATE_KEY="${{ github.event.inputs.tf_state_key }}"
          if [ -z "$TF_STATE_KEY" ]; then
            if [ "$DEPLOY_MODE" = "production" ]; then
              TF_STATE_KEY="${DEFAULT_TF_STATE_KEY}"
            else
              TF_STATE_KEY="${DEPLOY_MODE}/terraform.tfstate"
            fi
          fi
          if [ "$DEPLOY_MODE" = "production" ] && [ "$TF_STATE_KEY" = "production/terraform.tfstate" ]; then
            echo "Normalizing legacy production state key to ${DEFAULT_TF_STATE_KEY}."
            TF_STATE_KEY="${DEFAULT_TF_STATE_KEY}"
          fi

          echo "deploy_mode=${DEPLOY_MODE}" >> "$GITHUB_OUTPUT"
          echo "tf_state_key=${TF_STATE_KEY}" >> "$GITHUB_OUTPUT"
          TF_VARS_FILE="environments/${DEPLOY_MODE}.tfvars"
          if [ -f "$TF_VARS_FILE" ]; then
            echo "tf_vars_file=${TF_VARS_FILE}" >> "$GITHUB_OUTPUT"
          else
            echo "tf_vars_file=" >> "$GITHUB_OUTPUT"
          fi

      - name: Terraform Init
        run: terraform init -backend-config="key=${{ steps.context.outputs.tf_state_key }}"

      - name: Validate deploy tfvars profile
        if: ${{ steps.context.outputs.tf_vars_file == '' }}
        run: |
          echo "::error::Deploy mode '${{ steps.context.outputs.deploy_mode }}' requires infra/terraform/environments/${{ steps.context.outputs.deploy_mode }}.tfvars to avoid destructive default drift."
          exit 1

      - name: Validate production state key
        if: ${{ steps.context.outputs.deploy_mode == 'production' }}
        run: |
          if [ "${{ steps.context.outputs.tf_state_key }}" != "${DEFAULT_TF_STATE_KEY}" ]; then
            echo "::error::Production deploys must use ${DEFAULT_TF_STATE_KEY}. Got '${{ steps.context.outputs.tf_state_key }}'."
            exit 1
          fi

      - name: Validate manual force-unlock inputs
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.force_unlock == 'true' && github.event.inputs.force_unlock_id == '' }}
        run: |
          echo "::error::force_unlock=true requires force_unlock_id to be set."
          exit 1

      - name: Optional manual force-unlock
        if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.force_unlock == 'true' && github.event.inputs.force_unlock_id != '' }}
        run: |
          echo "Manual force-unlock requested for state key: ${{ steps.context.outputs.tf_state_key }}"
          terraform force-unlock -force "${{ github.event.inputs.force_unlock_id }}"

      - name: Prune unmanaged SES domain state (production)
        if: ${{ steps.context.outputs.deploy_mode == 'production' && steps.context.outputs.tf_vars_file != '' }}
        run: |
          # If production tfvars sets ses_domain="", SES identity resources are intentionally unmanaged.
          # Remove any legacy SES domain resources from state so plan does not fail on read permissions.
          if grep -Eq '^[[:space:]]*ses_domain[[:space:]]*=[[:space:]]*""[[:space:]]*$' "${{ steps.context.outputs.tf_vars_file }}"; then
            echo "ses_domain is empty in ${{ steps.context.outputs.tf_vars_file }}; pruning legacy SES domain resources from Terraform state (best effort)."
            terraform state rm 'aws_ses_domain_dkim.main[0]' || true
            terraform state rm 'aws_ses_domain_identity.main[0]' || true
          fi

      - name: Import existing ECS services
        env:
          # Import evaluates the Terraform configuration, so required vars must
          # be present here too (not only in terraform plan/apply).
          TF_VAR_environment: ${{ steps.context.outputs.deploy_mode }}
          TF_VAR_app_mode: ${{ steps.context.outputs.deploy_mode }}
          TF_VAR_app_env: ${{ steps.context.outputs.deploy_mode }}
          TF_VAR_secrets_manager_arn: ${{ secrets.SECRETS_MANAGER_ARN }}
          TF_VAR_db_password: ${{ secrets.DB_PASSWORD }}
        run: |
          # Best-effort import: If services exist but aren't in state, bring them in.
          # If they don't exist (fresh deploy) or are already in state, this fails safely (|| true).
          # The subsequent 'plan' will determine the actual required changes.
          IMPORT_ARGS=()
          if [ -n "${{ steps.context.outputs.tf_vars_file }}" ]; then
            IMPORT_ARGS+=("-var-file=${{ steps.context.outputs.tf_vars_file }}")
          fi
          PROJECT_NAME="${{ env.PROJECT_NAME }}"
          for svc in backend frontend worker beat; do
            echo "Attempting import for aws_ecs_service.${svc}..."
            terraform import "${IMPORT_ARGS[@]}" -lock-timeout=10m "aws_ecs_service.${svc}" "${PROJECT_NAME}-cluster/${PROJECT_NAME}-${svc}" || true
          done

      - name: Determine image tags
        id: tags
        run: |
          if [ -n "${{ needs.build-backend.outputs.image_tag }}" ]; then
            echo "backend_tag=${{ needs.build-backend.outputs.image_tag }}" >> "$GITHUB_OUTPUT"
          else
            echo "backend_tag=latest" >> "$GITHUB_OUTPUT"
          fi
          if [ -n "${{ needs.build-frontend.outputs.image_tag }}" ]; then
            echo "frontend_tag=${{ needs.build-frontend.outputs.image_tag }}" >> "$GITHUB_OUTPUT"
          else
            echo "frontend_tag=latest" >> "$GITHUB_OUTPUT"
          fi

      - name: Terraform Plan
        run: |
          PLAN_ARGS=()
          if [ -n "${{ steps.context.outputs.tf_vars_file }}" ]; then
            PLAN_ARGS+=("-var-file=${{ steps.context.outputs.tf_vars_file }}")
          fi
          run_plan() {
            terraform plan "${PLAN_ARGS[@]}" \
              -no-color \
              -lock-timeout=10m \
              -var="environment=${{ steps.context.outputs.deploy_mode }}" \
              -var="app_mode=${{ steps.context.outputs.deploy_mode }}" \
              -var="app_env=${{ steps.context.outputs.deploy_mode }}" \
              -var="backend_image_tag=${{ steps.tags.outputs.backend_tag }}" \
              -var="frontend_image_tag=${{ steps.tags.outputs.frontend_tag }}" \
              -var="secrets_manager_arn=${{ secrets.SECRETS_MANAGER_ARN }}" \
              -var="db_password=${{ secrets.DB_PASSWORD }}" \
              -out=tfplan
          }

          set +e
          run_plan 2>&1 | tee /tmp/terraform-plan.log
          PLAN_EXIT=${PIPESTATUS[0]}
          set -e

          if [ "$PLAN_EXIT" -eq 0 ]; then
            exit 0
          fi

          # Recovery path for stale DynamoDB lock rows from interrupted runs.
          if grep -q "Error acquiring the state lock" /tmp/terraform-plan.log; then
            LOCK_ID="$(grep -m1 "ID:" /tmp/terraform-plan.log | sed -E 's/.*ID:[[:space:]]*([^[:space:]]+).*/\1/')"
            if [ -n "$LOCK_ID" ]; then
              echo "Detected Terraform state lock ($LOCK_ID). Attempting force-unlock then one retry."
              terraform force-unlock -force "$LOCK_ID" || true
              run_plan
              exit 0
            fi
          fi

          exit "$PLAN_EXIT"

      - name: Terraform Apply
        run: terraform apply -lock-timeout=10m -auto-approve tfplan

  deploy:
    name: Deploy Services
    runs-on: ubuntu-latest
    needs: [changes, build-backend, build-frontend, terraform]
    if: always() && needs.terraform.result == 'success' && (needs.build-backend.result == 'success' || needs.build-frontend.result == 'success')
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Deploy updated backend
        if: needs.build-backend.result == 'success'
        run: |
          aws ecs update-service --cluster $PROJECT_NAME-cluster --service $PROJECT_NAME-backend --force-new-deployment
          aws ecs update-service --cluster $PROJECT_NAME-cluster --service $PROJECT_NAME-worker --force-new-deployment
          aws ecs update-service --cluster $PROJECT_NAME-cluster --service $PROJECT_NAME-beat --force-new-deployment

      - name: Deploy updated frontend
        if: needs.build-frontend.result == 'success'
        run: |
          aws ecs update-service --cluster $PROJECT_NAME-cluster --service $PROJECT_NAME-frontend --force-new-deployment

      - name: Wait for services to stabilize
        run: |
          aws ecs wait services-stable --cluster $PROJECT_NAME-cluster --services $PROJECT_NAME-backend $PROJECT_NAME-frontend $PROJECT_NAME-worker $PROJECT_NAME-beat
